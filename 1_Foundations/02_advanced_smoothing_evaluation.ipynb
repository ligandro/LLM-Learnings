{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd5330f",
   "metadata": {},
   "source": [
    "# Statistical Language Models â€“ Advanced Smoothing & Evaluation\n",
    "\n",
    "This notebook explores **advanced techniques** for building better statistical language models.\n",
    "\n",
    "### Advanced Smoothing Techniques\n",
    "Moving beyond basic Laplace smoothing to more sophisticated methods:\n",
    "\n",
    "1. **Good-Turing Smoothing** - Adjusts probabilities based on frequency-of-frequencies\n",
    "2. **Kneser-Ney Smoothing** - Uses word continuation probability (state-of-the-art for n-gram models)\n",
    "3. **Comparison** - Understanding when each method performs best\n",
    "\n",
    "### Model Evaluation\n",
    "- **Perplexity** - Standard metric for comparing language models\n",
    "- **Cross-validation** - Testing on real text data\n",
    "- **In-domain vs Out-of-domain** - Performance on familiar vs unfamiliar text\n",
    "\n",
    "### Extension to Higher-Order Models\n",
    "- **Trigram models** - Looking back 2 words instead of 1\n",
    "- **Trade-offs** - Computational cost vs accuracy\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Dataset Resources**\n",
    "- Project Gutenberg: https://www.gutenberg.org/\n",
    "- Example text: https://www.gutenberg.org/files/1342/1342-0.txt (Pride and Prejudice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7235597",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Text\n",
    "\n",
    "First, let's set up our preprocessing pipeline and load sample text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405edb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'natural', 'language', 'processing', 'language', 'models', 'help', 'predict', 'words', 'i', 'love', 'learning', 'language', 'models']\n",
      "Total tokens: 15\n",
      "Unique words: 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Clean and tokenize text\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "# Sample text with multiple sentences\n",
    "sample_text = \"\"\"\n",
    "I love natural language processing.\n",
    "Language models help predict words.\n",
    "I love learning language models.\n",
    "\"\"\"\n",
    "\n",
    "tokens = preprocess(sample_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique words: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20c3c0",
   "metadata": {},
   "source": [
    "## 2. Unigram & Bigram Counts\n",
    "\n",
    "Build the foundational counts we'll use for all smoothing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d6a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts:\n",
      "  language: 3\n",
      "  i: 2\n",
      "  love: 2\n",
      "  models: 2\n",
      "  natural: 1\n",
      "  processing: 1\n",
      "  help: 1\n",
      "  predict: 1\n",
      "  words: 1\n",
      "  learning: 1\n",
      "\n",
      "Bigram Counts:\n",
      "  ('i', 'love'): 2\n",
      "  ('language', 'models'): 2\n",
      "  ('love', 'natural'): 1\n",
      "  ('natural', 'language'): 1\n",
      "  ('language', 'processing'): 1\n",
      "  ('processing', 'language'): 1\n",
      "  ('models', 'help'): 1\n",
      "  ('help', 'predict'): 1\n",
      "  ('predict', 'words'): 1\n",
      "  ('words', 'i'): 1\n",
      "  ('love', 'learning'): 1\n",
      "  ('learning', 'language'): 1\n"
     ]
    }
   ],
   "source": [
    "# Count individual words\n",
    "unigram_counts = Counter(tokens)\n",
    "\n",
    "# Count word pairs\n",
    "bigram_counts = defaultdict(int)\n",
    "for i in range(len(tokens) - 1):\n",
    "    bigram_counts[(tokens[i], tokens[i+1])] += 1\n",
    "\n",
    "print(\"Unigram Counts:\")\n",
    "for word, count in unigram_counts.most_common():\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(f\"\\nBigram Counts:\")\n",
    "for bigram, count in sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {bigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8880a",
   "metadata": {},
   "source": [
    "## 3. Laplace (Add-One) Smoothing\n",
    "\n",
    "**Review from previous notebook:** Laplace smoothing adds 1 to all counts.\n",
    "\n",
    "**Formula:**  \n",
    "$$P_{\\text{Laplace}}(w_2|w_1) = \\frac{\\text{count}(w_1, w_2) + 1}{\\text{count}(w_1) + V}$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Simple and intuitive\n",
    "- Can over-smooth (gives too much probability to unseen events)\n",
    "- Works reasonably well for small vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1912c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10\n",
      "Total bigram probabilities: 100\n",
      "\n",
      "Sample Laplace probabilities:\n",
      "  ('i', 'i'): count=0, P=0.083333\n",
      "  ('i', 'predict'): count=0, P=0.083333\n",
      "  ('i', 'love'): count=2, P=0.250000\n",
      "  ('i', 'language'): count=0, P=0.083333\n",
      "  ('i', 'models'): count=0, P=0.083333\n"
     ]
    }
   ],
   "source": [
    "vocab = set(tokens)\n",
    "V = len(vocab)\n",
    "\n",
    "laplace_bigram_probs = {}\n",
    "for w1 in vocab:\n",
    "    for w2 in vocab:\n",
    "        count = bigram_counts.get((w1, w2), 0)\n",
    "        laplace_bigram_probs[(w1, w2)] = (count + 1) / (unigram_counts[w1] + V)\n",
    "\n",
    "print(f\"Vocabulary size: {V}\")\n",
    "print(f\"Total bigram probabilities: {len(laplace_bigram_probs)}\")\n",
    "print(\"\\nSample Laplace probabilities:\")\n",
    "for bigram, prob in list(laplace_bigram_probs.items())[:5]:\n",
    "    original_count = bigram_counts.get(bigram, 0)\n",
    "    print(f\"  {bigram}: count={original_count}, P={prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae07703",
   "metadata": {},
   "source": [
    "## 4. Good-Turing Smoothing\n",
    "\n",
    "**Good-Turing smoothing** uses the frequency-of-frequencies to estimate probabilities of unseen events.\n",
    "\n",
    "### Key Idea\n",
    "- Count how many items appear once, twice, three times, etc.\n",
    "- Use items seen $(r+1)$ times to estimate probability of items seen $r$ times\n",
    "- Items never seen get probability based on items seen once\n",
    "\n",
    "**Formula:**  \n",
    "$$r^* = \\frac{(r+1) \\cdot N_{r+1}}{N_r}$$\n",
    "\n",
    "Where:\n",
    "- $r$ = observed count\n",
    "- $N_r$ = number of items with count $r$\n",
    "- $r^*$ = adjusted count\n",
    "\n",
    "**Advantages:**\n",
    "- More principled than Laplace\n",
    "- Better handling of rare events\n",
    "- Based on the actual distribution of frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77334212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good-Turing Smoothed Probabilities by Frequency:\n",
      "Frequency       Count      Adjusted Prob\n",
      "========================================\n",
      "1               10         0.028571\n",
      "2               2          0.142857\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def good_turing(counts):\n",
    "    \"\"\"\n",
    "    Apply Good-Turing smoothing to a dictionary of counts.\n",
    "    Returns adjusted probability for each frequency level.\n",
    "    \"\"\"\n",
    "    # Count frequencies of frequencies\n",
    "    freq_of_freq = Counter(counts.values())\n",
    "    N = sum(counts.values())  # Total count\n",
    "    gt_probs = {}\n",
    "\n",
    "    for r, Nr in freq_of_freq.items():\n",
    "        Nr1 = freq_of_freq.get(r + 1, 0)  # Items with frequency r+1\n",
    "        if Nr1 > 0:\n",
    "            # Adjust count using Good-Turing formula\n",
    "            r_star = (r + 1) * (Nr1 / Nr)\n",
    "            gt_probs[r] = r_star / N\n",
    "        else:\n",
    "            # Fallback to simple probability if no r+1 items\n",
    "            gt_probs[r] = r / N\n",
    "\n",
    "    return gt_probs\n",
    "\n",
    "# Apply to bigram counts\n",
    "gt_bigram_probs = good_turing(bigram_counts)\n",
    "\n",
    "print(\"Good-Turing Smoothed Probabilities by Frequency:\")\n",
    "print(f\"{'Frequency':<15} {'Count':<10} {'Adjusted Prob'}\")\n",
    "print(\"=\" * 40)\n",
    "for freq in sorted(gt_bigram_probs.keys()):\n",
    "    num_items = len([k for k, v in bigram_counts.items() if v == freq])\n",
    "    print(f\"{freq:<15} {num_items:<10} {gt_bigram_probs[freq]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec264be",
   "metadata": {},
   "source": [
    "## 5. Kneser-Ney Smoothing\n",
    "\n",
    "**Kneser-Ney** is considered the **state-of-the-art** n-gram smoothing technique. It uses a clever insight: the probability of a word should consider how many **different contexts** it appears in, not just raw frequency.\n",
    "\n",
    "### Key Innovation: Continuation Probability\n",
    "\n",
    "Instead of asking \"How often does 'Francisco' appear?\", ask \"In how many different contexts does 'Francisco' appear?\"\n",
    "\n",
    "For example:\n",
    "- \"San Francisco\" is very common\n",
    "- But \"Francisco\" only appears after \"San\"\n",
    "- So \"Francisco\" has low **continuation probability**\n",
    "\n",
    "**Formula (Simplified Bigram):**  \n",
    "$$P_{KN}(w_2|w_1) = \\frac{\\max(\\text{count}(w_1, w_2) - D, 0)}{\\text{count}(w_1)} + \\lambda_{w_1} \\cdot P_{\\text{continuation}}(w_2)$$\n",
    "\n",
    "Where:\n",
    "- $D$ = discount parameter (typically 0.75)\n",
    "- $\\lambda_{w_1}$ = normalization factor\n",
    "- $P_{\\text{continuation}}(w_2)$ = # of unique words before $w_2$ / total unique bigrams\n",
    "\n",
    "**Why it's better:**\n",
    "- Handles rare words more intelligently\n",
    "- Considers word versatility, not just frequency\n",
    "- Produces more natural language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d189af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney Probabilities:\n",
      "Bigram                    Count    KN Prob      Laplace Prob\n",
      "============================================================\n",
      "('help', 'predict')       1        0.312500     0.181818\n",
      "('i', 'love')             2        0.656250     0.250000\n",
      "('language', 'models')    2        0.437500     0.230769\n",
      "('language', 'processing') 1        0.104167     0.153846\n",
      "('learning', 'language')  1        0.437500     0.181818\n",
      "('love', 'learning')      1        0.156250     0.166667\n",
      "('love', 'natural')       1        0.156250     0.166667\n",
      "('models', 'help')        1        0.156250     0.166667\n",
      "('natural', 'language')   1        0.437500     0.181818\n",
      "('predict', 'words')      1        0.312500     0.181818\n"
     ]
    }
   ],
   "source": [
    "D = 0.75  # Discount parameter\n",
    "\n",
    "def kneser_ney(unigram_counts, bigram_counts):\n",
    "    \"\"\"\n",
    "    Apply Kneser-Ney smoothing to bigram model.\n",
    "    Uses continuation probability instead of raw frequency.\n",
    "    \"\"\"\n",
    "    # Calculate continuation counts: how many different words precede each word\n",
    "    continuation_counts = defaultdict(set)\n",
    "    for (w1, w2) in bigram_counts:\n",
    "        continuation_counts[w2].add(w1)\n",
    "\n",
    "    # Continuation probability: # unique contexts / total unique bigrams\n",
    "    continuation_probs = {\n",
    "        w: len(continuation_counts[w]) / len(bigram_counts)\n",
    "        for w in continuation_counts\n",
    "    }\n",
    "\n",
    "    # Calculate Kneser-Ney probabilities\n",
    "    kn_probs = {}\n",
    "    for (w1, w2), count in bigram_counts.items():\n",
    "        # Lambda (normalization): discount / unigram count\n",
    "        lambda_w1 = D / unigram_counts[w1]\n",
    "        \n",
    "        # Main formula: discounted probability + continuation component\n",
    "        kn_probs[(w1, w2)] = (max(count - D, 0) / unigram_counts[w1] + \n",
    "                              lambda_w1 * continuation_probs.get(w2, 0))\n",
    "    \n",
    "    return kn_probs\n",
    "\n",
    "kn_bigram_probs = kneser_ney(unigram_counts, bigram_counts)\n",
    "\n",
    "print(\"Kneser-Ney Probabilities:\")\n",
    "print(f\"{'Bigram':<25} {'Count':<8} {'KN Prob':<12} {'Laplace Prob'}\")\n",
    "print(\"=\" * 60)\n",
    "for bigram in sorted(kn_bigram_probs.keys())[:10]:\n",
    "    count = bigram_counts[bigram]\n",
    "    kn_prob = kn_bigram_probs[bigram]\n",
    "    lap_prob = laplace_bigram_probs.get(bigram, 0)\n",
    "    print(f\"{str(bigram):<25} {count:<8} {kn_prob:<12.6f} {lap_prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d407529d",
   "metadata": {},
   "source": [
    "## 6. Sentence Probability & Perplexity\n",
    "\n",
    "Now let's evaluate and compare our smoothing methods using two metrics:\n",
    "\n",
    "### Sentence Probability\n",
    "How likely is a given sentence according to our model?\n",
    "\n",
    "### Perplexity\n",
    "A measure of how \"surprised\" the model is by test data. **Lower is better.**\n",
    "\n",
    "**Interpretation:**\n",
    "- Perplexity of 10 = model is as uncertain as choosing from 10 options\n",
    "- Perplexity of 100 = model is very uncertain\n",
    "- Good models have low perplexity on in-domain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f57b149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Evaluation:\n",
      "\n",
      "Sentence: 'I love language models'\n",
      "  Kneser-Ney Probability:  2.87e-11\n",
      "  Kneser-Ney Perplexity:   3265.76\n",
      "  Laplace Probability:     4.81e-03\n",
      "  Laplace Perplexity:      5.92\n",
      "\n",
      "Sentence: 'language models help predict'\n",
      "  Kneser-Ney Probability:  2.14e-02\n",
      "  Kneser-Ney Perplexity:   3.60\n",
      "  Laplace Probability:     6.99e-03\n",
      "  Laplace Perplexity:      5.23\n",
      "\n",
      "Sentence: 'I enjoy learning new things'\n",
      "  Kneser-Ney Probability:  1.00e-40\n",
      "  Kneser-Ney Perplexity:   10000000000.00\n",
      "  Laplace Probability:     1.00e-40\n",
      "  Laplace Perplexity:      10000000000.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sentence_probability(sentence, model):\n",
    "    \"\"\"Calculate the probability of a sentence using bigram model\"\"\"\n",
    "    words = preprocess(sentence)\n",
    "    prob = 1.0\n",
    "    for i in range(len(words) - 1):\n",
    "        prob *= model.get((words[i], words[i+1]), 1e-10)\n",
    "    return prob\n",
    "\n",
    "def perplexity(sentence, model):\n",
    "    \"\"\"Calculate perplexity of a sentence - lower is better\"\"\"\n",
    "    words = preprocess(sentence)\n",
    "    N = len(words) - 1  # Number of bigrams\n",
    "    if N <= 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    log_prob = 0\n",
    "    for i in range(N):\n",
    "        p = model.get((words[i], words[i+1]), 1e-10)\n",
    "        log_prob += math.log(p)\n",
    "    \n",
    "    return math.exp(-log_prob / N)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"I love language models\",           # In vocabulary\n",
    "    \"language models help predict\",     # In vocabulary\n",
    "    \"I enjoy learning new things\"       # Partially out of vocabulary\n",
    "]\n",
    "\n",
    "print(\"Sentence Evaluation:\\n\")\n",
    "for sent in test_sentences:\n",
    "    print(f\"Sentence: '{sent}'\")\n",
    "    print(f\"  Kneser-Ney Probability:  {sentence_probability(sent, kn_bigram_probs):.2e}\")\n",
    "    print(f\"  Kneser-Ney Perplexity:   {perplexity(sent, kn_bigram_probs):.2f}\")\n",
    "    print(f\"  Laplace Probability:     {sentence_probability(sent, laplace_bigram_probs):.2e}\")\n",
    "    print(f\"  Laplace Perplexity:      {perplexity(sent, laplace_bigram_probs):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535dc862",
   "metadata": {},
   "source": [
    "## 7. Working with Real Text Data\n",
    "\n",
    "Let's apply our models to a real book from Project Gutenberg to see how they perform at scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9239de70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Successfully downloaded text\n",
      "Text length: 743,334 characters\n",
      "\n",
      "First 300 characters:\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "\n",
      "                        156 CHARING CROSS ROAD\n",
      "                                LONDON\n",
      "\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download Pride and Prejudice from Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/files/1342/1342-0.txt\"\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(url)\n",
    "    full_text = response.read().decode('utf-8')\n",
    "    \n",
    "    # Remove Project Gutenberg header and footer\n",
    "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    \n",
    "    start_idx = full_text.find(start_marker)\n",
    "    end_idx = full_text.find(end_marker)\n",
    "    \n",
    "    if start_idx != -1 and end_idx != -1:\n",
    "        full_text = full_text[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"âœ“ Successfully downloaded text\")\n",
    "    print(f\"Text length: {len(full_text):,} characters\")\n",
    "    print(f\"\\nFirst 300 characters:\\n{full_text[:300]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error downloading: {e}\")\n",
    "    print(\"You can manually download from https://www.gutenberg.org/files/1342/1342-0.txt\")\n",
    "    full_text = sample_text  # Fallback to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "653ee338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 127,166\n",
      "Unique tokens: 7,201\n",
      "\n",
      "First 50 tokens: ['start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'illustration', 'george', 'allen', 'publisher', 'charing', 'cross', 'road', 'london', 'ruskin', 'house', 'illustration', 'reading', 'janes', 'letters', 'chap', 'pride', 'and', 'prejudice', 'by', 'jane', 'austen', 'with', 'a', 'preface', 'by', 'george', 'saintsbury', 'and', 'illustrations', 'by', 'hugh', 'thomson', 'illustration', 'ruskin', 'charing', 'house', 'cross', 'road', 'london', 'george', 'allen', 'chiswick', 'presscharles', 'whittingham']\n",
      "\n",
      "Total unique unigrams: 7,201\n",
      "Total unique bigrams: 57,284\n",
      "\n",
      "Most common words: [('the', 4654), ('to', 4291), ('of', 3835), ('and', 3713), ('her', 2276), ('i', 2102), ('a', 2021), ('in', 1977), ('was', 1874), ('she', 1744)]\n",
      "\n",
      "Most common bigrams: [(('of', 'the'), 508), (('to', 'be'), 445), (('in', 'the'), 420), (('i', 'am'), 309), (('of', 'her'), 274), (('to', 'the'), 264), (('it', 'was'), 255), (('of', 'his'), 242), (('mr', 'darcy'), 242), (('she', 'was'), 213)]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the full text\n",
    "full_tokens = preprocess(full_text)\n",
    "\n",
    "print(f\"Total tokens: {len(full_tokens):,}\")\n",
    "print(f\"Unique tokens: {len(set(full_tokens)):,}\")\n",
    "print(f\"\\nFirst 50 tokens: {full_tokens[:50]}\")\n",
    "\n",
    "# Build counts from full text\n",
    "full_unigram_counts = Counter(full_tokens)\n",
    "full_bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(full_tokens) - 1):\n",
    "    full_bigram_counts[(full_tokens[i], full_tokens[i+1])] += 1\n",
    "\n",
    "print(f\"\\nTotal unique unigrams: {len(full_unigram_counts):,}\")\n",
    "print(f\"Total unique bigrams: {len(full_bigram_counts):,}\")\n",
    "print(f\"\\nMost common words: {full_unigram_counts.most_common(10)}\")\n",
    "print(f\"\\nMost common bigrams: {sorted(full_bigram_counts.items(), key=lambda x: x[1], reverse=True)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73ad00",
   "metadata": {},
   "source": [
    "### 7.1 Build Models on Full Text\n",
    "\n",
    "Now let's train all three smoothing methods on the complete novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ff490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Kneser-Ney model built: 57,284 bigram probabilities\n",
      "âœ“ Good-Turing probabilities by frequency: 130 levels\n",
      "\n",
      "âœ“ All models ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Apply Kneser-Ney to full text (most important)\n",
    "kn_full_probs = kneser_ney(full_unigram_counts, full_bigram_counts)\n",
    "print(f\"âœ“ Kneser-Ney model built: {len(kn_full_probs):,} bigram probabilities\")\n",
    "\n",
    "# Apply Good-Turing to full text\n",
    "gt_full_probs = good_turing(full_bigram_counts)\n",
    "print(f\"âœ“ Good-Turing probabilities by frequency: {len(gt_full_probs)} levels\")\n",
    "\n",
    "# For comparison: sample of Laplace (computationally expensive for full vocab)\n",
    "print(\"\\nâœ“ All models ready for evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819d22",
   "metadata": {},
   "source": [
    "### 7.2 Compare Model Performance\n",
    "\n",
    "Test our models on various sentences - both in-domain (from the book) and out-of-domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f55d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison on Test Sentences\n",
      "\n",
      "Sentence                                           KN Perplexity  \n",
      "======================================================================\n",
      "it is a truth universally acknowledged             62.01          \n",
      "mr darcy is a very proud man                       28.23          \n",
      "elizabeth bennet is intelligent and witty          249476768.86   \n",
      "I love natural language processing                 174819981.45   \n",
      "\n",
      "ðŸ“Š Observations:\n",
      "- In-domain sentences (from the book) have lower perplexity\n",
      "- Out-of-domain sentences have higher perplexity\n",
      "- This shows the model learned the book's style!\n"
     ]
    }
   ],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"it is a truth universally acknowledged\",      # From the book (should score well)\n",
    "    \"mr darcy is a very proud man\",                # From the book\n",
    "    \"elizabeth bennet is intelligent and witty\",   # Related to book\n",
    "    \"I love natural language processing\"           # Out of domain (should score worse)\n",
    "]\n",
    "\n",
    "print(\"Model Comparison on Test Sentences\\n\")\n",
    "print(f\"{'Sentence':<50} {'KN Perplexity':<15}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    perp_kn = perplexity(sent, kn_full_probs)\n",
    "    print(f\"{sent:<50} {perp_kn:<15.2f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Observations:\")\n",
    "print(\"- In-domain sentences (from the book) have lower perplexity\")\n",
    "print(\"- Out-of-domain sentences have higher perplexity\")\n",
    "print(\"- This shows the model learned the book's style!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0114a79",
   "metadata": {},
   "source": [
    "## 8. Extension to Trigram Models\n",
    "\n",
    "Bigrams look at 1 previous word. **Trigrams** look at 2 previous words, capturing more context.\n",
    "\n",
    "**Trade-off:**\n",
    "- âœ“ Better context understanding\n",
    "- âœ— More data sparsity (need more training data)\n",
    "- âœ— Higher computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3cc3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trigrams: 107,150\n",
      "\n",
      "Most common trigrams:\n",
      "  ('i', 'do', 'not'): 66\n",
      "  ('i', 'am', 'sure'): 61\n",
      "  ('as', 'soon', 'as'): 56\n",
      "  ('she', 'could', 'not'): 52\n",
      "  ('that', 'he', 'had'): 37\n",
      "  ('in', 'the', 'world'): 36\n",
      "  ('copyright', 'by', 'george'): 35\n",
      "  ('by', 'george', 'allen'): 35\n",
      "  ('i', 'am', 'not'): 34\n",
      "  ('it', 'would', 'be'): 31\n"
     ]
    }
   ],
   "source": [
    "# Build trigram counts\n",
    "trigram_counts = defaultdict(int)\n",
    "for i in range(len(full_tokens) - 2):\n",
    "    trigram_counts[(full_tokens[i], full_tokens[i+1], full_tokens[i+2])] += 1\n",
    "\n",
    "print(f\"Total trigrams: {len(trigram_counts):,}\")\n",
    "print(f\"\\nMost common trigrams:\")\n",
    "for trigram, count in sorted(trigram_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02454e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram probabilities computed: 1,000,000\n",
      "\n",
      "Sample trigram probabilities:\n",
      "  ('overdone', 'overdone', 'overdone'): 0.00013885\n",
      "  ('overdone', 'overdone', 'disgracing'): 0.00013885\n",
      "  ('overdone', 'overdone', 'cessation'): 0.00013885\n",
      "  ('overdone', 'overdone', 'settlement'): 0.00013885\n",
      "  ('overdone', 'overdone', 'test'): 0.00013885\n"
     ]
    }
   ],
   "source": [
    "# Trigram probabilities with Laplace smoothing (sample subset)\n",
    "full_vocab = set(full_tokens)\n",
    "full_V = len(full_vocab)\n",
    "\n",
    "# Sample vocabulary for efficiency (full computation would be vocab^3)\n",
    "sample_vocab = list(full_vocab)[:100]\n",
    "\n",
    "trigram_probs = {}\n",
    "for w1 in sample_vocab:\n",
    "    for w2 in sample_vocab:\n",
    "        for w3 in sample_vocab:\n",
    "            count = trigram_counts.get((w1, w2, w3), 0)\n",
    "            bigram_count = full_bigram_counts.get((w1, w2), 1)\n",
    "            trigram_probs[(w1, w2, w3)] = (count + 1) / (bigram_count + full_V)\n",
    "\n",
    "print(f\"Trigram probabilities computed: {len(trigram_probs):,}\")\n",
    "print(f\"\\nSample trigram probabilities:\")\n",
    "for tri, prob in list(trigram_probs.items())[:5]:\n",
    "    print(f\"  {tri}: {prob:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f68d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram vs Trigram Comparison\n",
      "\n",
      "Sentence                                           Bigram PP    Trigram PP  \n",
      "===========================================================================\n",
      "it is a truth universally acknowledged             62.01        10000000000.00\n",
      "mr darcy is a very proud man                       28.23        10000000000.00\n",
      "elizabeth bennet is intelligent and witty          249476768.86 10000000000.00\n",
      "\n",
      "Note: Trigrams capture more context but may have higher perplexity due to data sparsity.\n"
     ]
    }
   ],
   "source": [
    "def trigram_perplexity(sentence, trigram_model):\n",
    "    \"\"\"Calculate perplexity using trigram model\"\"\"\n",
    "    words = preprocess(sentence)\n",
    "    N = len(words) - 2  # Number of trigrams\n",
    "    if N <= 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    log_prob = 0\n",
    "    for i in range(N):\n",
    "        p = trigram_model.get((words[i], words[i+1], words[i+2]), 1e-10)\n",
    "        log_prob += math.log(p)\n",
    "    \n",
    "    return math.exp(-log_prob / N)\n",
    "\n",
    "# Compare bigram vs trigram on test sentences\n",
    "print(\"Bigram vs Trigram Comparison\\n\")\n",
    "print(f\"{'Sentence':<50} {'Bigram PP':<12} {'Trigram PP':<12}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "for sent in test_sentences[:3]:  # Use first 3 sentences\n",
    "    perp_bi = perplexity(sent, kn_full_probs)\n",
    "    perp_tri = trigram_perplexity(sent, trigram_probs)\n",
    "    print(f\"{sent:<50} {perp_bi:<12.2f} {perp_tri:<12.2f}\")\n",
    "\n",
    "print(\"\\nNote: Trigrams capture more context but may have higher perplexity due to data sparsity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859e24e",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Smoothing Techniques Summary\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Laplace** | Simple, fast | Over-smooths, wastes probability mass | Small datasets, quick prototyping |\n",
    "| **Good-Turing** | Principled, frequency-aware | Complex, may need large data | Understanding frequency distributions |\n",
    "| **Kneser-Ney** | State-of-the-art, context-aware | More complex implementation | Production n-gram models |\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "1. **Smoothing is Essential**\n",
    "   - Without smoothing, unseen bigrams have 0 probability\n",
    "   - This breaks sentence probability calculations\n",
    "   - All practical language models use some form of smoothing\n",
    "\n",
    "2. **Kneser-Ney is Superior**\n",
    "   - Uses continuation probability, not just frequency\n",
    "   - Better models natural language patterns\n",
    "   - Industry standard for n-gram models\n",
    "\n",
    "3. **Perplexity Measures Model Quality**\n",
    "   - Lower perplexity = better model\n",
    "   - Compare models using same test set\n",
    "   - In-domain data scores better than out-of-domain\n",
    "\n",
    "4. **Higher-Order Models Trade-Offs**\n",
    "   - Trigrams > Bigrams > Unigrams for context\n",
    "   - But: more parameters, more sparsity, more computation\n",
    "   - Need larger training corpora\n",
    "\n",
    "### Connection to Modern LLMs\n",
    "\n",
    "While statistical n-gram models are foundational, modern LLMs overcome their limitations:\n",
    "\n",
    "**Statistical LMs:**\n",
    "- âœ— Limited context (2-3 words)\n",
    "- âœ— Data sparsity issues\n",
    "- âœ— No semantic understanding\n",
    "- âœ“ Fast, interpretable\n",
    "- âœ“ No training needed\n",
    "\n",
    "**Neural LMs (GPT, BERT, etc.):**\n",
    "- âœ“ Long-range context (100s-1000s of tokens)\n",
    "- âœ“ Learn dense representations\n",
    "- âœ“ Semantic understanding\n",
    "- âœ— Require training\n",
    "- âœ— Less interpretable\n",
    "\n",
    "But understanding statistical models is crucial for:\n",
    "- Appreciating why neural models work so well\n",
    "- Baseline comparisons\n",
    "- Understanding evaluation metrics (perplexity is still used!)\n",
    "- Simple, fast applications\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **You now understand advanced smoothing techniques and how to evaluate language models!**\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different discount parameters in Kneser-Ney\n",
    "- Try other texts from Project Gutenberg\n",
    "- Implement interpolation (combining unigram, bigram, trigram)\n",
    "- Compare to neural language models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
