{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f6fda9",
   "metadata": {},
   "source": [
    "# Statistical Language Models \n",
    "\n",
    "This notebook covers the fundamental concepts of **Statistical Language Models**, which form the foundation of modern NLP and LLMs.\n",
    "\n",
    "Statistical Language Models assign probabilities to sequences of words, helping us understand:\n",
    "- How likely a sentence is in a given language\n",
    "- Which word should come next in a sequence\n",
    "- How to evaluate model performance\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Unigram Models** - Single word probabilities\n",
    "2. **Bigram Models** - Word pair probabilities  \n",
    "3. **Sentence Probability** - Computing likelihood of sequences\n",
    "4. **Smoothing Techniques** - Handling unseen word combinations\n",
    "5. **Perplexity** - Evaluating model quality\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ **Sample Dataset Resources**\n",
    "- Project Gutenberg texts: https://www.gutenberg.org/\n",
    "- Example file: https://www.gutenberg.org/files/1342/1342-0.txt (Pride and Prejudice)\n",
    "\n",
    "You can download a text file and place it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff583745",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Text\n",
    "\n",
    "Text preprocessing is the first critical step in building a language model. We need to:\n",
    "- **Normalize** the text (convert to lowercase)\n",
    "- **Remove** special characters and punctuation\n",
    "- **Tokenize** the text into individual words\n",
    "\n",
    "This creates a clean, standardized corpus for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d683705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I love learning NLP and I love coding\n",
      "Tokens: ['i', 'love', 'learning', 'nlp', 'and', 'i', 'love', 'coding']\n",
      "Number of tokens: 8\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing non-alphabetic characters\n",
    "    3. Splitting into tokens (words)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "# Sample text for demonstration\n",
    "sample_text = \"I love learning NLP and I love coding\"\n",
    "tokens = preprocess(sample_text)\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80570bd",
   "metadata": {},
   "source": [
    "## 2. Unigram Language Model\n",
    "\n",
    "A **unigram model** treats each word independently and calculates the probability of a word based on its frequency in the corpus.\n",
    "\n",
    "**Formula:**  \n",
    "$$P(w) = \\frac{\\text{count}(w)}{\\text{total words}}$$\n",
    "\n",
    "This is the simplest language model where:\n",
    "- Each word's probability is independent of context\n",
    "- Probabilities sum to 1 across the vocabulary\n",
    "- More frequent words have higher probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c675361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts:\n",
      "  i: 2\n",
      "  love: 2\n",
      "  learning: 1\n",
      "  nlp: 1\n",
      "  and: 1\n",
      "  coding: 1\n",
      "\n",
      "Total tokens: 8\n",
      "\n",
      "Unigram Probabilities:\n",
      "  P(i) = 0.2500\n",
      "  P(love) = 0.2500\n",
      "  P(learning) = 0.1250\n",
      "  P(nlp) = 0.1250\n",
      "  P(and) = 0.1250\n",
      "  P(coding) = 0.1250\n",
      "\n",
      "Sum of probabilities: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Count each word's occurrences\n",
    "unigram_counts = Counter(tokens)\n",
    "total_tokens = sum(unigram_counts.values())\n",
    "\n",
    "# Calculate probability for each word\n",
    "unigram_probs = {w: c / total_tokens for w, c in unigram_counts.items()}\n",
    "\n",
    "print(\"Unigram Counts:\")\n",
    "for word, count in unigram_counts.items():\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal tokens: {total_tokens}\")\n",
    "print(\"\\nUnigram Probabilities:\")\n",
    "for word, prob in unigram_probs.items():\n",
    "    print(f\"  P({word}) = {prob:.4f}\")\n",
    "\n",
    "# Verify probabilities sum to 1\n",
    "print(f\"\\nSum of probabilities: {sum(unigram_probs.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e29223",
   "metadata": {},
   "source": [
    "## 3. Bigram Language Model\n",
    "\n",
    "A **bigram model** considers word pairs (consecutive words) and calculates the probability of a word given the previous word.\n",
    "\n",
    "**Formula:**  \n",
    "$$P(w_2 | w_1) = \\frac{\\text{count}(w_1, w_2)}{\\text{count}(w_1)}$$\n",
    "\n",
    "This captures:\n",
    "- **Context dependency** - the probability of a word depends on the previous word\n",
    "- **Local patterns** - common word sequences in the language\n",
    "- **Better predictions** than unigram models for natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0939b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Counts:\n",
      "  ('i', 'love'): 2\n",
      "  ('love', 'learning'): 1\n",
      "  ('learning', 'nlp'): 1\n",
      "  ('nlp', 'and'): 1\n",
      "  ('and', 'i'): 1\n",
      "  ('love', 'coding'): 1\n",
      "\n",
      "Bigram Probabilities:\n",
      "  P(love | i) = 1.0000\n",
      "  P(learning | love) = 0.5000\n",
      "  P(nlp | learning) = 1.0000\n",
      "  P(and | nlp) = 1.0000\n",
      "  P(i | and) = 1.0000\n",
      "  P(coding | love) = 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Count bigram occurrences\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):\n",
    "    bigrams[(tokens[i], tokens[i+1])] += 1\n",
    "\n",
    "print(\"Bigram Counts:\")\n",
    "for bigram, count in bigrams.items():\n",
    "    print(f\"  {bigram}: {count}\")\n",
    "\n",
    "# Calculate conditional probabilities P(w2|w1)\n",
    "bigram_probs = {}\n",
    "for (w1, w2), count in bigrams.items():\n",
    "    bigram_probs[(w1, w2)] = count / unigram_counts[w1]\n",
    "\n",
    "print(\"\\nBigram Probabilities:\")\n",
    "for bigram, prob in bigram_probs.items():\n",
    "    print(f\"  P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84ebce",
   "metadata": {},
   "source": [
    "## 4. Sentence Probability (Bigram Model)\n",
    "\n",
    "To calculate the probability of an entire sentence, we use the **chain rule** with bigram probabilities:\n",
    "\n",
    "**Formula:**  \n",
    "$$P(\\text{sentence}) = P(w_1) \\times P(w_2|w_1) \\times P(w_3|w_2) \\times ... \\times P(w_n|w_{n-1})$$\n",
    "\n",
    "This tells us:\n",
    "- How likely a sentence is according to our language model\n",
    "- Which sentences are more \"natural\" in the language\n",
    "- Can be used for tasks like sentence ranking or text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c329ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Probabilities:\n",
      "  'I love coding': 0.125000\n",
      "  'I love learning': 0.125000\n",
      "  'coding love I': 0.000000\n"
     ]
    }
   ],
   "source": [
    "def sentence_probability(sentence, unigram_probs, bigram_probs):\n",
    "    \"\"\"\n",
    "    Calculate the probability of a sentence using bigram model.\n",
    "    P(sentence) = P(w1) * P(w2|w1) * P(w3|w2) * ...\n",
    "    \"\"\"\n",
    "    words = preprocess(sentence)\n",
    "    \n",
    "    # Start with first word probability\n",
    "    prob = unigram_probs.get(words[0], 0)\n",
    "    \n",
    "    # Multiply by conditional probabilities\n",
    "    for i in range(len(words) - 1):\n",
    "        prob *= bigram_probs.get((words[i], words[i+1]), 0)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Test with different sentences\n",
    "test_sentences = [\n",
    "    \"I love coding\",\n",
    "    \"I love learning\",\n",
    "    \"coding love I\"  # Unnatural word order\n",
    "]\n",
    "\n",
    "print(\"Sentence Probabilities:\")\n",
    "for sent in test_sentences:\n",
    "    prob = sentence_probability(sent, unigram_probs, bigram_probs)\n",
    "    print(f\"  '{sent}': {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ba186",
   "metadata": {},
   "source": [
    "## 5. Laplace (Add-One) Smoothing\n",
    "\n",
    "**The Problem:** What happens when we encounter a bigram we've never seen before? The probability is 0, which makes the entire sentence probability 0!\n",
    "\n",
    "**The Solution:** **Smoothing** techniques add a small probability to unseen events.\n",
    "\n",
    "### Laplace (Add-One) Smoothing\n",
    "\n",
    "**Formula:**  \n",
    "$$P_{\\text{smoothed}}(w_2|w_1) = \\frac{\\text{count}(w_1, w_2) + 1}{\\text{count}(w_1) + V}$$\n",
    "\n",
    "Where $V$ is the vocabulary size.\n",
    "\n",
    "This ensures:\n",
    "- No zero probabilities for unseen bigrams\n",
    "- All bigrams have at least a small probability\n",
    "- Better generalization to new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379affcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6\n",
      "Vocabulary: {'nlp', 'i', 'and', 'learning', 'coding', 'love'}\n",
      "\n",
      "Comparison (Original vs Smoothed):\n",
      "  ('i', 'love'):\n",
      "    Original: 1.000000\n",
      "    Smoothed: 0.375000\n",
      "  ('love', 'learning'):\n",
      "    Original: 0.500000\n",
      "    Smoothed: 0.250000\n",
      "  ('coding', 'nlp'):\n",
      "    Original: 0.000000\n",
      "    Smoothed: 0.142857\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "vocab = set(tokens)\n",
    "V = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {V}\")\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "\n",
    "# Apply Laplace smoothing to all possible bigrams\n",
    "smoothed_bigram_probs = {}\n",
    "for w1 in vocab:\n",
    "    for w2 in vocab:\n",
    "        count = bigrams.get((w1, w2), 0)\n",
    "        smoothed_bigram_probs[(w1, w2)] = (count + 1) / (unigram_counts[w1] + V)\n",
    "\n",
    "# Compare original vs smoothed probabilities\n",
    "print(\"\\nComparison (Original vs Smoothed):\")\n",
    "sample_bigrams = [('i', 'love'), ('love', 'learning'), ('coding', 'nlp')]  # Last one is unseen\n",
    "\n",
    "for bg in sample_bigrams:\n",
    "    orig_prob = bigram_probs.get(bg, 0.0)\n",
    "    smooth_prob = smoothed_bigram_probs.get(bg, 0.0)\n",
    "    print(f\"  {bg}:\")\n",
    "    print(f\"    Original: {orig_prob:.6f}\")\n",
    "    print(f\"    Smoothed: {smooth_prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029d2ec",
   "metadata": {},
   "source": [
    "## 6. Perplexity Calculation\n",
    "\n",
    "**Perplexity** is a standard metric for evaluating language models. It measures how \"surprised\" the model is by the test data.\n",
    "\n",
    "**Formula:**  \n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i|w_{i-1})\\right)$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Lower perplexity** = better model (less surprised by the data)\n",
    "- **Higher perplexity** = worse model (more uncertain)\n",
    "- Perplexity of $k$ means the model is as uncertain as if it had to choose uniformly from $k$ possibilities\n",
    "\n",
    "Example: A perplexity of 50 means the model is as uncertain as if it had to randomly choose from 50 words at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6444009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Scores (using smoothed model):\n",
      "  'I love learning': 3.27\n",
      "  'I love coding': 3.27\n",
      "  'learning and coding': 7.00\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(sentence, bigram_probs):\n",
    "    \"\"\"\n",
    "    Calculate perplexity of a sentence using the bigram model.\n",
    "    Lower perplexity = better model fit\n",
    "    \"\"\"\n",
    "    words = preprocess(sentence)\n",
    "    N = len(words) - 1  # Number of bigrams\n",
    "    log_prob = 0\n",
    "\n",
    "    # Sum log probabilities to avoid numerical underflow\n",
    "    for i in range(N):\n",
    "        prob = bigram_probs.get((words[i], words[i+1]), 1e-10)  # Small value for unseen bigrams\n",
    "        log_prob += math.log(prob)\n",
    "\n",
    "    # Calculate perplexity\n",
    "    return math.exp(-log_prob / N)\n",
    "\n",
    "# Test on various sentences\n",
    "test_sentences = [\n",
    "    \"I love learning\",\n",
    "    \"I love coding\",\n",
    "    \"learning and coding\"\n",
    "]\n",
    "\n",
    "print(\"Perplexity Scores (using smoothed model):\")\n",
    "for sent in test_sentences:\n",
    "    pp = perplexity(sent, smoothed_bigram_probs)\n",
    "    print(f\"  '{sent}': {pp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3141607",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Unigram Models** \n",
    "   - Simplest model: treats words independently\n",
    "   - Good baseline but ignores context\n",
    "\n",
    "2. **Bigram Models**\n",
    "   - Captures local context (previous word)\n",
    "   - Much better for natural language\n",
    "   - Can be extended to trigrams, n-grams\n",
    "\n",
    "3. **Smoothing**\n",
    "   - Essential for handling unseen word combinations\n",
    "   - Laplace smoothing is simple but effective\n",
    "   - Other methods: Good-Turing, Kneser-Ney\n",
    "\n",
    "4. **Perplexity**\n",
    "   - Standard evaluation metric\n",
    "   - Lower is better\n",
    "   - Allows comparison between models\n",
    "\n",
    "### Limitations of Statistical LMs\n",
    "\n",
    "- **Data sparsity**: Need huge corpora for good coverage\n",
    "- **Limited context**: Bigrams only see 1 word back\n",
    "- **No semantic understanding**: Purely based on co-occurrence\n",
    "- **Storage**: Need to store counts for all n-grams\n",
    "\n",
    "### Modern Evolution\n",
    "\n",
    "Today's LLMs (GPT, BERT, etc.) overcome these limitations through:\n",
    "- **Neural networks**: Learn distributed representations\n",
    "- **Long-range context**: Attention mechanisms see entire sequences\n",
    "- **Semantic understanding**: Learn meaning, not just co-occurrence\n",
    "- **Efficient storage**: Parameters instead of explicit counts\n",
    "\n",
    "But statistical LMs remain important for understanding the foundations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
