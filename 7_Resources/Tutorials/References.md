# LLM Learning Resources & References

A curated collection of resources for learning about Large Language Models.

---

## üìö Online Courses

### Fundamentals
- **Stanford CS224N: NLP with Deep Learning** - [Link]
- **Hugging Face NLP Course** - [Link]
- **Fast.ai Practical Deep Learning** - [Link]
- **DeepLearning.AI LLM Courses** - [Link]

### Advanced
- **Stanford CS324: Large Language Models** - [Link]
- **Berkeley Deep Learning Course** - [Link]
- **MIT Introduction to Deep Learning** - [Link]

---

## üìñ Research Papers

### Foundational Papers
- **Attention Is All You Need (Transformer)** - Vaswani et al., 2017 - [Link]
- **BERT: Pre-training of Deep Bidirectional Transformers** - Devlin et al., 2018 - [Link]
- **GPT: Improving Language Understanding by Generative Pre-Training** - Radford et al., 2018 - [Link]
- **GPT-2: Language Models are Unsupervised Multitask Learners** - Radford et al., 2019 - [Link]
- **GPT-3: Language Models are Few-Shot Learners** - Brown et al., 2020 - [Link]

### Recent Advances
- **LLaMA: Open and Efficient Foundation Language Models** - Touvron et al., 2023 - [Link]
- **Mistral 7B** - Jiang et al., 2023 - [Link]
- **Constitutional AI** - Anthropic, 2022 - [Link]
- **InstructGPT: Training language models to follow instructions** - OpenAI, 2022 - [Link]

### Fine-tuning & Optimization
- **LoRA: Low-Rank Adaptation of Large Language Models** - Hu et al., 2021 - [Link]
- **QLoRA: Efficient Finetuning of Quantized LLMs** - Dettmers et al., 2023 - [Link]
- **Prefix-Tuning** - Li and Liang, 2021 - [Link]
- **PEFT: Parameter-Efficient Fine-Tuning** - [Link]

### Retrieval Augmented Generation (RAG)
- **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** - Lewis et al., 2020 - [Link]
- **REALM: Retrieval-Augmented Language Model Pre-Training** - Guu et al., 2020 - [Link]

---

## üéì Tutorials & Guides

### Getting Started
- **Hugging Face Transformers Documentation** - [Link]
- **LangChain Documentation** - [Link]
- **LlamaIndex Documentation** - [Link]
- **OpenAI API Documentation** - [Link]
- **Anthropic Claude Documentation** - [Link]

### Practical Guides
- **How to Train Your Own LLM** - [Link]
- **Fine-tuning Large Language Models** - [Link]
- **Prompt Engineering Guide** - [Link]
- **Building RAG Applications** - [Link]
- **LLM Agent Development** - [Link]

### Video Tutorials
- **Andrej Karpathy: Neural Networks Zero to Hero** - [Link]
- **3Blue1Brown: Neural Networks Series** - [Link]
- **StatQuest: Machine Learning Fundamentals** - [Link]

---

## üõ†Ô∏è Tools & Frameworks

### Model Libraries
- **Hugging Face Transformers** - [Link]
- **PyTorch** - [Link]
- **TensorFlow** - [Link]
- **JAX** - [Link]

### LLM Frameworks
- **LangChain** - Framework for LLM applications - [Link]
- **LlamaIndex** - Data framework for LLMs - [Link]
- **Semantic Kernel** - Microsoft's LLM framework - [Link]
- **AutoGPT** - Autonomous AI agents - [Link]

### Fine-tuning & Training
- **PEFT (Hugging Face)** - Parameter-efficient fine-tuning - [Link]
- **Axolotl** - Fine-tuning tool - [Link]
- **DeepSpeed** - Distributed training - [Link]
- **Accelerate** - PyTorch training acceleration - [Link]

### Inference & Deployment
- **vLLM** - Fast LLM inference - [Link]
- **Text Generation Inference (TGI)** - Hugging Face inference server - [Link]
- **Ollama** - Run LLMs locally - [Link]
- **llama.cpp** - C++ LLM inference - [Link]

### Vector Databases
- **Chroma** - Open-source embeddings database - [Link]
- **Pinecone** - Vector database service - [Link]
- **Weaviate** - Vector search engine - [Link]
- **FAISS** - Facebook AI Similarity Search - [Link]
- **Qdrant** - Vector search engine - [Link]

### Evaluation
- **HELM** - Holistic Evaluation of Language Models - [Link]
- **EleutherAI LM Evaluation Harness** - [Link]
- **BERTScore** - Evaluation metric - [Link]

---

## üì∞ Blogs & Articles

### Company Blogs
- **OpenAI Blog** - [Link]
- **Anthropic Research** - [Link]
- **Google AI Blog** - [Link]
- **Meta AI Blog** - [Link]
- **Hugging Face Blog** - [Link]

### Technical Blogs
- **Jay Alammar's Blog** - Visual explanations of NLP/ML - [Link]
- **Sebastian Raschka's Blog** - ML & Deep Learning - [Link]
- **Lilian Weng's Blog** - AI Research - [Link]
- **The Gradient** - AI research and perspectives - [Link]

### Community Resources
- **Papers with Code** - [Link]
- **ArXiv Sanity** - [Link]
- **Reddit r/MachineLearning** - [Link]
- **Towards Data Science** - [Link]

---

## üéØ Datasets

### Pre-training & General
- **The Pile** - Large-scale text dataset - [Link]
- **Common Crawl** - Web archive - [Link]
- **Wikipedia Dumps** - [Link]
- **BookCorpus** - [Link]
- **Project Gutenberg** - Free ebooks - [Link]

### Fine-tuning
- **Alpaca Dataset** - Instruction-following - [Link]
- **FLAN Dataset** - Instruction tuning - [Link]
- **Dolly Dataset** - High-quality instructions - [Link]
- **ShareGPT** - Conversational data - [Link]

### Benchmarks
- **GLUE Benchmark** - [Link]
- **SuperGLUE** - [Link]
- **MMLU** - Massive Multitask Language Understanding - [Link]
- **HellaSwag** - Commonsense reasoning - [Link]
- **TruthfulQA** - [Link]

---

## üèÜ Model Collections

### Open Source Models
- **Hugging Face Model Hub** - [Link]
- **LLaMA Models** - Meta's foundation models - [Link]
- **Mistral AI Models** - [Link]
- **Falcon Models** - [Link]
- **MPT Models** - MosaicML - [Link]

### Model Leaderboards
- **Open LLM Leaderboard** - Hugging Face - [Link]
- **Chatbot Arena** - LMSYS - [Link]
- **AlpacaEval Leaderboard** - [Link]

---

## üé§ Conferences & Workshops

### Major Conferences
- **NeurIPS** - Neural Information Processing Systems - [Link]
- **ICML** - International Conference on Machine Learning - [Link]
- **ICLR** - International Conference on Learning Representations - [Link]
- **ACL** - Association for Computational Linguistics - [Link]
- **EMNLP** - Empirical Methods in NLP - [Link]

---

## üì± Social Media & Communities

### Twitter/X Accounts to Follow
- **@karpathy** - Andrej Karpathy
- **@ylecun** - Yann LeCun
- **@goodfellow_ian** - Ian Goodfellow
- **@AndrewYNg** - Andrew Ng
- **@huggingface** - Hugging Face

### Discord Communities
- **Hugging Face Discord** - [Link]
- **EleutherAI Discord** - [Link]
- **LangChain Discord** - [Link]

### GitHub Organizations
- **Hugging Face** - [Link]
- **EleutherAI** - [Link]
- **Stanford CRFM** - [Link]

---

## üìö Books

### Fundamentals
- **Deep Learning** - Goodfellow, Bengio, Courville - [Link]
- **Speech and Language Processing** - Jurafsky & Martin - [Link]
- **Natural Language Processing with Transformers** - Tunstall et al. - [Link]

### Advanced
- **Dive into Deep Learning** - Zhang et al. - [Link]
- **Probabilistic Machine Learning** - Kevin Murphy - [Link]

---

## üí° Tips for Learning

1. **Start with Foundations** - Understand transformers and attention mechanisms
2. **Hands-on Practice** - Implement models using Hugging Face
3. **Read Papers** - Start with foundational papers, then recent advances
4. **Join Communities** - Engage on Discord, Reddit, Twitter
5. **Build Projects** - Apply learnings to real-world problems
6. **Stay Updated** - Follow blogs, newsletters, and conferences

---

**Last Updated:** December 19, 2025

**Note:** This is a template. Add actual links as you discover valuable resources!